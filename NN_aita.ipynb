{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import random\n",
    "import string\n",
    "from collections import defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as Data\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import word_tokenize \n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from tqdm import tqdm, notebook\n",
    "\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'aita_raw.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>title</th>\n",
       "      <th>body</th>\n",
       "      <th>edited</th>\n",
       "      <th>verdict</th>\n",
       "      <th>score</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1ytr72</td>\n",
       "      <td>1.393275e+09</td>\n",
       "      <td>[AITA] Construction worker here</td>\n",
       "      <td>I have been on a parking structure project for...</td>\n",
       "      <td>False</td>\n",
       "      <td>too close to call</td>\n",
       "      <td>63</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1ytxov</td>\n",
       "      <td>1.393279e+09</td>\n",
       "      <td>[AITA] I wrote an explanation in TIL and came ...</td>\n",
       "      <td>[Here is the post in question](http://www.redd...</td>\n",
       "      <td>False</td>\n",
       "      <td>asshole</td>\n",
       "      <td>52</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1yu29c</td>\n",
       "      <td>1.393281e+09</td>\n",
       "      <td>[AITA] Threw my parent's donuts away</td>\n",
       "      <td>My parents are diabetic, morbidly obese, and a...</td>\n",
       "      <td>1393290576.0</td>\n",
       "      <td>asshole</td>\n",
       "      <td>140</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1yu41e</td>\n",
       "      <td>1.393282e+09</td>\n",
       "      <td>[AITA] I Put My Empty Beer on a Bar Table</td>\n",
       "      <td>Relevant Facts:\\n\\n1) It was a crowded bar, th...</td>\n",
       "      <td>False</td>\n",
       "      <td>nothing happened</td>\n",
       "      <td>45</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1yu8hi</td>\n",
       "      <td>1.393285e+09</td>\n",
       "      <td>I told a goth girl she looked like a clown.</td>\n",
       "      <td>I was four.</td>\n",
       "      <td>False</td>\n",
       "      <td>not the asshole</td>\n",
       "      <td>74</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167755</th>\n",
       "      <td>ex970f</td>\n",
       "      <td>1.580577e+09</td>\n",
       "      <td>AITA for telling my husband to f* off after he...</td>\n",
       "      <td>My husband (28M) and I (32F) are married for a...</td>\n",
       "      <td>1580584475.0</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>1373</td>\n",
       "      <td>304.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167756</th>\n",
       "      <td>ex97ye</td>\n",
       "      <td>1.580577e+09</td>\n",
       "      <td>AITA for refusing to give my ticket to my brot...</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>False</td>\n",
       "      <td>no a-holes here</td>\n",
       "      <td>4</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167758</th>\n",
       "      <td>ex9dwo</td>\n",
       "      <td>1.580578e+09</td>\n",
       "      <td>AITA for attempting to keep my students out of...</td>\n",
       "      <td>Upfront apologies for formatting. I’m also try...</td>\n",
       "      <td>False</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>4</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167759</th>\n",
       "      <td>ex9egs</td>\n",
       "      <td>1.580578e+09</td>\n",
       "      <td>WIBTA if I left my brothers fate up to the state?</td>\n",
       "      <td>A little back story my mom is a drug addict an...</td>\n",
       "      <td>False</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>280</td>\n",
       "      <td>140.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167760</th>\n",
       "      <td>ex9g78</td>\n",
       "      <td>1.580578e+09</td>\n",
       "      <td>WIBTA for rocking the boat at work because my ...</td>\n",
       "      <td>I’m a (23F) apprentice in the trades. I work o...</td>\n",
       "      <td>False</td>\n",
       "      <td>not the a-hole</td>\n",
       "      <td>5</td>\n",
       "      <td>21.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>129953 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            id     timestamp  \\\n",
       "0       1ytr72  1.393275e+09   \n",
       "1       1ytxov  1.393279e+09   \n",
       "2       1yu29c  1.393281e+09   \n",
       "3       1yu41e  1.393282e+09   \n",
       "4       1yu8hi  1.393285e+09   \n",
       "...        ...           ...   \n",
       "167755  ex970f  1.580577e+09   \n",
       "167756  ex97ye  1.580577e+09   \n",
       "167758  ex9dwo  1.580578e+09   \n",
       "167759  ex9egs  1.580578e+09   \n",
       "167760  ex9g78  1.580578e+09   \n",
       "\n",
       "                                                    title  \\\n",
       "0                         [AITA] Construction worker here   \n",
       "1       [AITA] I wrote an explanation in TIL and came ...   \n",
       "2                    [AITA] Threw my parent's donuts away   \n",
       "3               [AITA] I Put My Empty Beer on a Bar Table   \n",
       "4             I told a goth girl she looked like a clown.   \n",
       "...                                                   ...   \n",
       "167755  AITA for telling my husband to f* off after he...   \n",
       "167756  AITA for refusing to give my ticket to my brot...   \n",
       "167758  AITA for attempting to keep my students out of...   \n",
       "167759  WIBTA if I left my brothers fate up to the state?   \n",
       "167760  WIBTA for rocking the boat at work because my ...   \n",
       "\n",
       "                                                     body        edited  \\\n",
       "0       I have been on a parking structure project for...         False   \n",
       "1       [Here is the post in question](http://www.redd...         False   \n",
       "2       My parents are diabetic, morbidly obese, and a...  1393290576.0   \n",
       "3       Relevant Facts:\\n\\n1) It was a crowded bar, th...         False   \n",
       "4                                             I was four.         False   \n",
       "...                                                   ...           ...   \n",
       "167755  My husband (28M) and I (32F) are married for a...  1580584475.0   \n",
       "167756                                          [deleted]         False   \n",
       "167758  Upfront apologies for formatting. I’m also try...         False   \n",
       "167759  A little back story my mom is a drug addict an...         False   \n",
       "167760  I’m a (23F) apprentice in the trades. I work o...         False   \n",
       "\n",
       "                  verdict  score  num_comments  \n",
       "0       too close to call     63           9.0  \n",
       "1                 asshole     52          13.0  \n",
       "2                 asshole    140          27.0  \n",
       "3        nothing happened     45           7.0  \n",
       "4         not the asshole     74          15.0  \n",
       "...                   ...    ...           ...  \n",
       "167755     not the a-hole   1373         304.0  \n",
       "167756    no a-holes here      4          16.0  \n",
       "167758     not the a-hole      4          15.0  \n",
       "167759     not the a-hole    280         140.0  \n",
       "167760     not the a-hole      5          21.0  \n",
       "\n",
       "[129953 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_path)\n",
    "df = df.dropna(axis=0)\n",
    "df['verdict'] = df['verdict'].str.lower()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not the a-hole           75393\n",
      "asshole                  27449\n",
      "no a-holes here          15129\n",
      "everyone sucks            7417\n",
      "not enough info           2713\n",
      "                         ...  \n",
      "not the a-hole (meta)        1\n",
      "should've towed              1\n",
      "not the slave                1\n",
      "not not a-hole               1\n",
      "rant                         1\n",
      "Name: verdict, Length: 410, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "counts = df['verdict'].value_counts()\n",
    "print(counts)\n",
    "v = df['verdict']\n",
    "df = df[v.replace(counts.gt(300))]\n",
    "replace_dict = {\n",
    "    'not the asshole': 'not the a-hole',\n",
    "    'no a--holes here': 'no a-holes here'\n",
    "}\n",
    "df = df.replace({'verdict': replace_dict})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not the a-hole     75766\n",
       "asshole            27449\n",
       "no a-holes here    15432\n",
       "everyone sucks      7417\n",
       "not enough info     2713\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not the a-hole    91198\n",
       "asshole           34866\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = df['verdict']\n",
    "df = df[v.replace(counts.gt(3000))]\n",
    "replace_dict = {\n",
    "    'no a-holes here': 'not the a-hole',\n",
    "    'everyone sucks': 'asshole'\n",
    "}\n",
    "df = df.replace({'verdict': replace_dict})\n",
    "df['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "not the a-hole    71536\n",
       "asshole           26863\n",
       "Name: verdict, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = df['body']\n",
    "df = df[v != '[deleted]']\n",
    "df['verdict'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asshole' 'not the a-hole']\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "verdicts = df['verdict'].unique()\n",
    "print(verdicts)\n",
    "num_classes = len(verdicts)\n",
    "print(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LemmaTokenizer:\n",
    "    ignore_tokens = string.punctuation\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, doc):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(doc) \n",
    "                if not any(i in t for i in self.ignore_tokens) and len(t) > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['edited'] = [0 if 'false' in d else 1 for d in df['edited'].str.lower() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_comments'] = df['num_comments'].astype(int)\n",
    "df['score'] = df['score'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['body'] = df['body'].str.lower()\n",
    "df['body'] = df['body'].str.replace('[^\\w\\s]', ' ')\n",
    "df['body'] = df['body'].str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df['verdict'].unique()\n",
    "verdict_map = dict(zip(classes, range(len(classes))))\n",
    "df['verdict'] = df['verdict'].replace(verdict_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train, df_temp = train_test_split(df, test_size=0.2, stratify=df['verdict'])\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.5, stratify=df_temp['verdict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_data = df_train[['edited', 'score', 'num_comments']].to_numpy()\n",
    "labels = df_train['verdict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=LemmaTokenizer()\n",
    "word_list = ['aita']\n",
    "\n",
    "stop_words = set(ENGLISH_STOP_WORDS.union(word_list))\n",
    "tokenized_stop = tokenizer(' '.join(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78719, 1000)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=1000, sublinear_tf=True, max_df=0.95, \n",
    "                             min_df=5, ngram_range=(1, 2), tokenizer=tokenizer, stop_words=tokenized_stop)\n",
    "text_features = vectorizer.fit_transform(df_train['body']).toarray()\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2231.68 MiB, increment: -27.46 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(78719, 1003)\n"
     ]
    }
   ],
   "source": [
    "text_features = np.array(np.hstack((transformed_data, text_features)))\n",
    "print(text_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2236.47 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1003\n",
      "(78719, 1003)\n",
      "(78719,)\n"
     ]
    }
   ],
   "source": [
    "X_train = text_features\n",
    "y_train = np.array(labels)\n",
    "input_size = len(X_train[0])\n",
    "print(input_size)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(42980, 1003)\n",
      "(42980,)\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "X_resampled, y_resampled = rus.fit_resample(X_train, y_train)\n",
    "print(X_resampled.shape)\n",
    "print(y_resampled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SelectKBest(k=500, score_func=<function chi2 at 0x7f1e6fbae9e0>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_rank = SelectKBest(chi2, k=500)\n",
    "feature_rank.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.001124e+05</td>\n",
       "      <td>abusive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.191386e+05</td>\n",
       "      <td>absolutely</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.651446e+02</td>\n",
       "      <td>able</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>7.679568e+01</td>\n",
       "      <td>emotional</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>3.969713e+01</td>\n",
       "      <td>money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>2.923383e-05</td>\n",
       "      <td>car</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>238</th>\n",
       "      <td>7.657551e-06</td>\n",
       "      <td>don care</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>3.289485e-06</td>\n",
       "      <td>social medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>690</th>\n",
       "      <td>1.845324e-06</td>\n",
       "      <td>question</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>4.812903e-07</td>\n",
       "      <td>awkward</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0              1\n",
       "2    2.001124e+05        abusive\n",
       "1    1.191386e+05     absolutely\n",
       "0    5.651446e+02           able\n",
       "268  7.679568e+01      emotional\n",
       "580  3.969713e+01          money\n",
       "..            ...            ...\n",
       "114  2.923383e-05            car\n",
       "238  7.657551e-06       don care\n",
       "806  3.289485e-06  social medium\n",
       "690  1.845324e-06       question\n",
       "58   4.812903e-07        awkward\n",
       "\n",
       "[1000 rows x 2 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = []\n",
    "for i, (score, feature) in enumerate(zip(feature_rank.scores_, feature_names)):\n",
    "    feat.append((score, feature))\n",
    "    \n",
    "dfObj = pd.DataFrame(feat) \n",
    "dfObj.sort_values(by=[0], ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9840, 1003)\n",
      "(9840,)\n"
     ]
    }
   ],
   "source": [
    "transformed_val = df_val[['edited', 'score', 'num_comments']].to_numpy()\n",
    "labels_val = df_val['verdict']\n",
    "X_val = np.array(np.hstack((transformed_val, vectorizer.transform(df_val['body']).toarray())))\n",
    "y_val = np.array(labels_val)\n",
    "del transformed_val\n",
    "print(X_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2650.51 MiB, increment: -0.31 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process = make_pipeline(PowerTransformer(method='yeo-johnson'))\n",
    "# ct = ColumnTransformer([(\"power\", process, [x for x in range(0, 6)])], remainder='passthrough')\n",
    "# X_train = ct.fit_transform(X_train)\n",
    "# X_test = ct.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2650.51 MiB, increment: 0.00 MiB\n"
     ]
    }
   ],
   "source": [
    "%memit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Data.TensorDataset(Variable(torch.from_numpy(X_train.astype(np.float32))), \n",
    "                                   Variable(torch.from_numpy(y_train.astype(np.long))))\n",
    "test_dataset = Data.TensorDataset(Variable(torch.from_numpy(X_val.astype(np.float32))), \n",
    "                                  Variable(torch.from_numpy(y_val.astype(np.long))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainHelper():\n",
    "    '''\n",
    "    Helper class that makes it a bit easier and cleaner to define the training routine\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    def __init__(self,model,train_set,test_set,opts):\n",
    "        self.model = model \n",
    "        \n",
    "        self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.epochs = opts['epochs']\n",
    "        self.optimizer = torch.optim.AdamW(model.parameters(), lr=opts['lr'], weight_decay=opts['weight_decay']) # optimizer method for gradient descent\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()                      # loss function\n",
    "        self.train_loader = torch.utils.data.DataLoader(dataset=train_set,\n",
    "                                                        batch_size=opts['batch_size'],\n",
    "                                                        shuffle=True)\n",
    "        self.test_loader = torch.utils.data.DataLoader(dataset=test_set,\n",
    "                                                       batch_size=opts['batch_size'],\n",
    "                                                       shuffle=True)\n",
    "        \n",
    "    def train(self):\n",
    "        self.model.train() #put model in training mode\n",
    "        self.train_accuracy = []\n",
    "\n",
    "        for epoch in range(self.epochs):\n",
    "            self.tr_loss = []\n",
    "            for i, (data,labels) in notebook.tqdm(enumerate(self.train_loader),\n",
    "                                                   total = len(self.train_loader)):\n",
    "\n",
    "                data, labels = data.to(self.device),labels.to(self.device)\n",
    "                self.optimizer.zero_grad()  \n",
    "                outputs = self.model(data)  \n",
    "                loss = self.criterion(outputs, labels) \n",
    "                loss.backward()                        \n",
    "                self.optimizer.step()                  \n",
    "                self.tr_loss.append(loss.item())\n",
    "                y_pred_softmax = torch.log_softmax(outputs.data, dim = 1)\n",
    "                _, predicted = torch.max(y_pred_softmax, dim = 1)\n",
    "                self.train_accuracy.append((predicted == labels).sum().item() / predicted.size(0))\n",
    "\n",
    "            \n",
    "            self.test(epoch) # run through the validation set\n",
    "        \n",
    "    def test(self,epoch):\n",
    "            \n",
    "            self.model.eval()    # puts model in eval mode - not necessary for this demo but good to know\n",
    "            self.test_loss = []\n",
    "            self.test_accuracy = []\n",
    "            \n",
    "            for i, (data, labels) in enumerate(self.test_loader):\n",
    "                \n",
    "                data, labels = data.to(self.device),labels.to(self.device)\n",
    "                \n",
    "                # pass data through network\n",
    "                # turn off gradient calculation to speed up calcs and reduce memory\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(data)\n",
    "                \n",
    "                # make our predictions and update our loss info\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                self.test_loss.append(loss.item())\n",
    "                y_pred_softmax = torch.log_softmax(outputs.data, dim = 1)\n",
    "                _, predicted = torch.max(y_pred_softmax, dim = 1)\n",
    "                self.test_accuracy.append((predicted == labels).sum().item() / predicted.size(0))\n",
    "            \n",
    "            print('epoch: {}, train loss: {}, test loss: {}, train accuracy: {}, test accuracy: {}'.format( \n",
    "                  epoch+1, np.mean(self.tr_loss), np.mean(self.test_loss), \n",
    "                  np.mean(self.train_accuracy), np.mean(self.test_accuracy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, input_size, classes):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(input_size, 2000)\n",
    "        self.relu1 = torch.nn.ReLU()\n",
    "        self.drop1 = torch.nn.Dropout(0.8)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(2000)\n",
    "\n",
    "        self.fc2 = torch.nn.Linear(2000, 1000)\n",
    "        self.relu2 = torch.nn.ReLU()\n",
    "        self.drop2 = torch.nn.Dropout(0.5)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(1000)\n",
    "\n",
    "\n",
    "        self.fc3 = torch.nn.Linear(1000, 500)\n",
    "        self.relu3 = torch.nn.ReLU()\n",
    "        self.drop3 = torch.nn.Dropout(0.2)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(500)\n",
    "\n",
    "\n",
    "        self.fc4 = torch.nn.Linear(500, classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.drop1(x)\n",
    "        x = self.bn1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.drop2(x)\n",
    "        x = self.bn2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.drop3(x)\n",
    "        x = self.bn3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(input_size=input_size, classes=num_classes)\n",
    "opts = {\n",
    "    'lr': 1e-4,\n",
    "    'weight_decay': 4e-3,\n",
    "    'epochs': 5,\n",
    "    'batch_size': 64\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "NetTrainer = TrainHelper(model = model,\n",
    "                      train_set = train_dataset,\n",
    "                      test_set = test_dataset,opts = opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0502e52cb01e4142909108809f8b72f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 1, train loss: 0.5991532109374923, test loss: 0.5666547814359912, train accuracy: 0.705955002258356, test accuracy: 0.739752435064935\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b631dd4d20142c4b3526cd13f5d281f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 2, train loss: 0.5555298519570653, test loss: 0.5343112395955371, train accuracy: 0.7272387969415408, test accuracy: 0.746178300865801\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c218f167c556434d82bc22d98edde985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 3, train loss: 0.5212569423322755, test loss: 0.5292709787170609, train accuracy: 0.7357220824192369, test accuracy: 0.7483089826839827\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32b44b48d2c6473fa2c22b4504757bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 4, train loss: 0.5236331029151513, test loss: 0.5249236887925631, train accuracy: 0.7404690322944896, test accuracy: 0.7516910173160173\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0b7e4e038ab4f1db55c9f7bab2c495a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1230.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "epoch: 5, train loss: 0.5080004737144563, test loss: 0.5310277557605273, train accuracy: 0.7438558604336044, test accuracy: 0.7505073051948052\n"
     ]
    }
   ],
   "source": [
    "NetTrainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'predict_aita_nn.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Net(input_size=input_size, classes=num_classes)\n",
    "# model.load_state_dict(torch.load(model_path))\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_datum(datum):\n",
    "    datum['early_access'] = int(datum['early_access'])\n",
    "    datum['hours'] = np.log1p(datum['hours']) / max_hours if datum['hours'] > 0 else 0\n",
    "    datum['text'] = datum['text'].lower()\n",
    "    datum['text'] = datum['text'].replace('[^\\w\\s]', ' ')\n",
    "    datum['text'] = datum['text'].replace('\\d+', ' ')\n",
    "    time = pd.to_datetime(datum['date'])\n",
    "    time_tran = [(np.sin(2 * np.pi* time.month / 12) + 1) / 2, (np.cos(2 * np.pi* time.month / 12) + 1) / 2, \n",
    "                   (np.sin(2 * np.pi* time.day / 31) + 1) / 2, (np.cos(2 * np.pi* time.day / 31) + 1) / 2]\n",
    "    del time\n",
    "    \n",
    "    transformed_datum = np.array([datum['early_access'], datum['hours'], *time_tran]).reshape((1, -1))\n",
    "    del time_tran\n",
    "    feat = vectorizer.transform([datum['text']]).toarray()\n",
    "    transformed_datum = np.array(np.hstack((transformed_datum, feat))).astype(np.float32)\n",
    "    del feat\n",
    "#     transformed_datum = ct.transform(transformed_datum)\n",
    "\n",
    "    return transformed_datum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readJSON(path):\n",
    "    for l in gzip.open(path, 'rt'):\n",
    "        d = eval(l)\n",
    "        u = d['userID']\n",
    "        try:\n",
    "            g = d['gameID']\n",
    "        except Exception as e:\n",
    "            g = None\n",
    "        yield u,g,d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10000it [00:21, 455.89it/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = open(\"predictions_Category.txt\", 'w')\n",
    "predictions.write(\"userID-reviewID,prediction\\n\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "for u,_,d in tqdm(readJSON(test_path)):\n",
    "    revID = d['reviewID']\n",
    "    d = clean_datum(d)\n",
    "    d = torch.from_numpy(d).to(device)\n",
    "    out = model(d)\n",
    "    y_pred_softmax = torch.log_softmax(out.data, dim = 1)\n",
    "    _, pred = torch.max(y_pred_softmax, dim = 1)\n",
    "    pred = pred.item()\n",
    "    \n",
    "    predictions.write(f'{u}-{revID},{pred}\\n')\n",
    "\n",
    "predictions.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
